"""LLM-based Judge for evaluating AI outputs against requirements."""

from __future__ import annotations
import json
from typing import Any, Optional
from pydantic import BaseModel


class JudgeResult(BaseModel):
    """Result from the LLM Judge evaluation."""

    passed: bool
    reason: str
    score: float  # 0.0 to 1.0


class LLMJudge:
    """Impartial Judge that evaluates AI outputs against requirements using an LLM."""

    SYSTEM_PROMPT = """You are an Impartial Judge and QA Tester.
Your Goal: Determine if an AI's response meets the user's requirements.
Input:
1. "Requirement": The specific goal or assertion (e.g., "Must generate valid JSON", "Must not mention competitors").
2. "Output": The actual response generated by the AI.
Your Task:
Evaluate the Output against the Requirement.
- Be effortless and strict.
- Ignore polite formatting if the core data is wrong.
Output Format:
Return JSON:
{
  "passed": boolean,
  "reason": "String explaining the verdict",
  "score": float (0.0 to 1.0)
}"""

    def __init__(self, executor: Optional[Any] = None):
        """Initialize with an optional executor for LLM calls."""
        self.executor = executor

    def evaluate(self, requirement: str, output: str) -> JudgeResult:
        """Evaluate the output against the requirement.

        Args:
            requirement: The specific requirement or assertion to check.
            output: The AI-generated output to evaluate.

        Returns:
            JudgeResult with passed status, reason, and score.
        """
        if self.executor is None:
            # Mock evaluation for testing without LLM
            return self._mock_evaluate(requirement, output)

        # Build the prompt for the judge
        user_prompt = f"""Requirement: {requirement}

Output:
{output}

Evaluate the output against the requirement and return your verdict as JSON."""

        try:
            response = self.executor.execute(
                prompt=f"{self.SYSTEM_PROMPT}\n\n{user_prompt}",
                config={"temperature": 0.0},  # Deterministic for consistency
            )

            # Parse JSON response
            result = self._parse_response(response)
            return result

        except Exception as e:
            return JudgeResult(passed=False, reason=f"Judge evaluation error: {str(e)}", score=0.0)

    def _parse_response(self, response: str) -> JudgeResult:
        """Parse the LLM response into a JudgeResult."""
        try:
            # Try to extract JSON from the response
            # Handle cases where LLM wraps JSON in markdown code blocks
            clean_response = response.strip()
            if clean_response.startswith("```"):
                # Remove markdown code block
                lines = clean_response.split("\n")
                clean_response = "\n".join(lines[1:-1])

            data = json.loads(clean_response)
            return JudgeResult(
                passed=bool(data.get("passed", False)),
                reason=str(data.get("reason", "No reason provided")),
                score=float(data.get("score", 0.0 if not data.get("passed") else 1.0)),
            )
        except json.JSONDecodeError:
            # If parsing fails, try to infer from text
            lower_response = response.lower()
            passed = "passed" in lower_response or "pass" in lower_response
            return JudgeResult(
                passed=passed,
                reason=f"Could not parse JSON response: {response[:200]}",
                score=1.0 if passed else 0.0,
            )

    def _mock_evaluate(self, requirement: str, output: str) -> JudgeResult:
        """Mock evaluation for testing without an actual LLM."""
        # Simple heuristic: check if requirement keywords appear in output
        req_lower = requirement.lower()
        out_lower = output.lower()

        # Check for basic patterns
        if "must contain" in req_lower:
            # Extract what should be contained
            target = req_lower.split("must contain")[-1].strip().strip("\"'")
            passed = target in out_lower
            return JudgeResult(
                passed=passed,
                reason=f"Output {'contains' if passed else 'does not contain'} '{target}'",
                score=1.0 if passed else 0.0,
            )

        if "must not" in req_lower or "should not" in req_lower:
            # Negation check - assume pass if nothing obvious
            return JudgeResult(
                passed=True,
                reason="Mock evaluation: negation requirements assumed passed",
                score=0.8,
            )

        if "valid json" in req_lower:
            try:
                json.loads(output)
                return JudgeResult(passed=True, reason="Output is valid JSON", score=1.0)
            except json.JSONDecodeError:
                return JudgeResult(passed=False, reason="Output is not valid JSON", score=0.0)

        # Default: optimistic mock
        return JudgeResult(
            passed=True,
            reason="Mock evaluation: no specific check implemented for this requirement",
            score=0.7,
        )
